{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Table of Content\n",
    "\n",
    "[1. Logistic Regression Codes](#lgr1) <br>\n",
    "\n",
    "[2. Comparison with Scikit-Learn](#lgr2) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='lgr1'></a> 1. Logistic Regression Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(z):\n",
    "    \"\"\"\n",
    "    The implementaion of sigmoid function\n",
    "    Inputs: z - numpy.ndarray, the intermediate linear results\n",
    "    Outputs: y_hat - numpy.ndarray, the mapped probaility by sigmoid\n",
    "    \"\"\"\n",
    "    y_hat = 1/(1 + np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "def y_hat_calc(X, b, W):\n",
    "    \"\"\"\n",
    "    Calculate the predicted outputs, y_hat\n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "    Outputs: y_hat.T - numpy.ndarray, the transposed predicted probaility outputs \n",
    "    \"\"\"\n",
    "    z = np.dot(W.T, X) + b\n",
    "    y_hat = sigmoid_func(z)\n",
    "    return y_hat.T\n",
    "    \n",
    "def cross_entropy_loss(Y, y_hat):\n",
    "    \"\"\"\n",
    "    Find the cross-entropy loss between true outputs and predicted outputs\n",
    "    Inputs: Y - numpy.ndarray, the true output values\n",
    "            y_hat - numpy.ndarray, the predicted output values\n",
    "    Outputs: L - numpy.ndarray, the cross-entropy loss\n",
    "    \"\"\"\n",
    "    L = - np.sum( np.dot(Y.T, np.log(y_hat)) + np.dot((1 - Y).T, np.log(1 - y_hat)) ) / Y.shape[0]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, b, W):\n",
    "    '''\n",
    "    Calculate the predicted class based on default threshold 0.5\n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "    Outputs: numpy.ndarray, the predicted class, 1 or 0\n",
    "    '''\n",
    "    return np.where(y_hat_calc(X, b, W) >= 0.5, 1, 0)\n",
    "\n",
    "def predict_accuracy(Y, X, b, W):\n",
    "    '''\n",
    "    Calculate the accuracy between true and predicted outputs\n",
    "    Inputs: Y - numpy.ndarray, the true output values\n",
    "            X - numpy.ndarray, the input feature values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "    Outputs: float, in range of [0.0, 1.0]\n",
    "    '''\n",
    "    return np.sum(Y == predict_class(X, b, W)) / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias_weights(X, Y, y_hat, b, W, learning_rate):\n",
    "    \"\"\"\n",
    "    Update the bias and weights based on Gradient Descent \n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            Y - numpy.ndarray, the true output values\n",
    "            y_hat - numpy.ndarray, the predicted output values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "            learning_rate - float, the learning rate used in Gradient Descent\n",
    "    Outputs: (b, W) - tuple, the updated bias and weights\n",
    "    \"\"\"\n",
    "    dL_db = np.sum(y_hat - Y) / X.shape[1]\n",
    "    dL_dW = np.dot(X, (y_hat-Y)) / X.shape[1] \n",
    "    \n",
    "    b = b - dL_db * learning_rate\n",
    "    W = W - dL_dW * learning_rate\n",
    "    \n",
    "    return (b, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, b, W, learning_rate, learning_iterations):\n",
    "    \"\"\"\n",
    "    Train logistic regression model for the specified iterations\n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            Y - numpy.ndarray, the true output values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "            learning_rate - float, the learning rate used in Gradient Descent\n",
    "            learning_iterations - int, the number of times of training\n",
    "    Outputs: (loss_history, b, W) - tuple, return the loss_history, and \n",
    "                                            the final bias and weights\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for i in range(learning_iterations):\n",
    "        y_hat = y_hat_calc(X, b, W)\n",
    "        \n",
    "        b, W = update_bias_weights(X, Y, y_hat, b, W, learning_rate)\n",
    "        \n",
    "        # find loss after the ith iteration of updating bias and weights\n",
    "        L = cross_entropy_loss(Y, y_hat_calc(X, b, W))\n",
    "        loss_history.append(L)\n",
    "        \n",
    "        if i < 5 or i >= learning_iterations-5:\n",
    "            print (\"iter={:d} \\t b={:.5f} \\t W1={:.5f} \\t loss={}\".format(i+1, b, W[0][0], L))\n",
    "\n",
    "    return (loss_history, b, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_bedrooms  is_easy_sell\n",
      "0             5             1\n",
      "1             4             1\n",
      "2             2             0 \n",
      "\n",
      "iter=1 \t b=0.01667 \t W1=0.11667 \t loss=0.581276460843159\n",
      "iter=2 \t b=0.02245 \t W1=0.18911 \t loss=0.5376940143963633\n",
      "iter=3 \t b=0.02215 \t W1=0.23701 \t loss=0.5182287998922913\n",
      "iter=4 \t b=0.01827 \t W1=0.27058 \t loss=0.5082855303264386\n",
      "iter=5 \t b=0.01213 \t W1=0.29518 \t loss=0.5025481850638057\n",
      "iter=499996 \t b=-24.16480 \t W1=8.15819 \t loss=0.00020014127130367002\n",
      "iter=499997 \t b=-24.16480 \t W1=8.15820 \t loss=0.00020014087078947866\n",
      "iter=499998 \t b=-24.16481 \t W1=8.15820 \t loss=0.00020014047027695344\n",
      "iter=499999 \t b=-24.16481 \t W1=8.15820 \t loss=0.00020014006976616837\n",
      "iter=500000 \t b=-24.16482 \t W1=8.15820 \t loss=0.00020013966925682737\n"
     ]
    }
   ],
   "source": [
    "# The small dataset\n",
    "data = np.array([[5,1],\n",
    "                 [4,1],\n",
    "                 [2,0]])\n",
    "col_names = ['num_bedrooms', 'is_easy_sell']\n",
    "print(pd.DataFrame(data, columns=col_names), \"\\n\")\n",
    "\n",
    "X = data[:, :-1] # all rows, all columns except the last column\n",
    "Y = data[:, -1]  # all rows, last column only\n",
    "\n",
    "#X, max_min_vals = max_min_norm(X) # normalize the input features\n",
    "X = X.T\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "# Initialize bias and weights\n",
    "initial_b = 0\n",
    "initial_W1 = 0\n",
    "\n",
    "# Set learing rate and iterations\n",
    "learning_rate = 0.1\n",
    "learning_iterations = 500000\n",
    "\n",
    "# Start the training of logistic regression model\n",
    "loss_history, b, W = train(X, Y, initial_b, np.array([[initial_W1]]), learning_rate, learning_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
