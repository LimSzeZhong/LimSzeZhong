{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee7238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f71067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(z):\n",
    "    \"\"\"\n",
    "    The implementaion of sigmoid function\n",
    "    Inputs: z - numpy.ndarray, the intermediate linear results\n",
    "    Outputs: y_hat - numpy.ndarray, the mapped probaility by sigmoid\n",
    "    \"\"\"\n",
    "    y_hat = 1/(1 + np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "def y_hat_calc(X, b, W):\n",
    "    \"\"\"\n",
    "    Calculate the predicted outputs, y_hat\n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "    Outputs: y_hat.T - numpy.ndarray, the transposed predicted probaility outputs \n",
    "    \"\"\"\n",
    "    z = np.dot(W.T, X) + b\n",
    "    y_hat = sigmoid_func(z)\n",
    "    return y_hat.T\n",
    "    \n",
    "def cross_entropy_loss(Y, y_hat):\n",
    "    \"\"\"\n",
    "    Find the cross-entropy loss between true outputs and predicted outputs\n",
    "    Inputs: Y - numpy.ndarray, the true output values\n",
    "            y_hat - numpy.ndarray, the predicted output values\n",
    "    Outputs: L - numpy.ndarray, the cross-entropy loss\n",
    "    \"\"\"\n",
    "    L = - np.sum( np.dot(Y.T, np.log(y_hat)) + np.dot((1 - Y).T, np.log(1 - y_hat)) ) / Y.shape[0]\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0986da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, b, W):\n",
    "    '''\n",
    "    Calculate the predicted class based on default threshold 0.5\n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "    Outputs: numpy.ndarray, the predicted class, 1 or 0\n",
    "    '''\n",
    "    return np.where(y_hat_calc(X, b, W) >= 0.5, 1, 0)\n",
    "\n",
    "def predict_accuracy(Y, X, b, W):\n",
    "    '''\n",
    "    Calculate the accuracy between true and predicted outputs\n",
    "    Inputs: Y - numpy.ndarray, the true output values\n",
    "            X - numpy.ndarray, the input feature values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "    Outputs: float, in range of [0.0, 1.0]\n",
    "    '''\n",
    "    return np.sum(Y == predict_class(X, b, W)) / Y.shape[0] #y.shape[0] is the number of samples m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c91acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bias_weights(X, Y, y_hat, b, W, learning_rate):\n",
    "    \"\"\"\n",
    "    Update the bias and weights based on Gradient Descent \n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            Y - numpy.ndarray, the true output values\n",
    "            y_hat - numpy.ndarray, the predicted output values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "            learning_rate - float, the learning rate used in Gradient Descent\n",
    "    Outputs: (b, W) - tuple, the updated bias and weights\n",
    "    \"\"\"\n",
    "    dL_db = np.sum(y_hat - Y) / X.shape[1]\n",
    "    dL_dW = np.dot(X, (y_hat-Y)) / X.shape[1] \n",
    "    \n",
    "    b = b - dL_db * learning_rate\n",
    "    W = W - dL_dW * learning_rate\n",
    "    \n",
    "    return (b, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb7e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, b, W, learning_rate, learning_iterations):\n",
    "    \"\"\"\n",
    "    Train logistic regression model for the specified iterations\n",
    "    Inputs: X - numpy.ndarray, the input feature values\n",
    "            Y - numpy.ndarray, the true output values\n",
    "            b - float/int, bias\n",
    "            W - numpy.ndarray, weights of features\n",
    "            learning_rate - float, the learning rate used in Gradient Descent\n",
    "            learning_iterations - int, the number of times of training\n",
    "    Outputs: (loss_history, b, W) - tuple, return the loss_history, and \n",
    "                                            the final bias and weights\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for i in range(learning_iterations):\n",
    "        y_hat = y_hat_calc(X, b, W)\n",
    "        \n",
    "        b, W = update_bias_weights(X, Y, y_hat, b, W, learning_rate)\n",
    "        \n",
    "        # find loss after the ith iteration of updating bias and weights\n",
    "        L = cross_entropy_loss(Y, y_hat_calc(X, b, W))\n",
    "        loss_history.append(L)\n",
    "        \n",
    "        if i < 5 or i >= learning_iterations-5:\n",
    "            print (\"iter={:d} \\t b={:.5f} \\t W={:.5f} \\t loss={}\".format(i+1, b, W[0][0], L))\n",
    "\n",
    "    return (loss_history, b, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba19abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_bedrooms  is_easy_sell\n",
      "0             5             1\n",
      "1             4             1\n",
      "2             2             0 \n",
      "\n",
      "iter=1 \t b=0.05000 \t W=0.11667 \t loss=1.738435597796271\n",
      "iter=2 \t b=0.06500 \t W=0.18625 \t loss=1.6179138087527978\n",
      "iter=3 \t b=0.06199 \t W=0.23174 \t loss=1.564784723513836\n",
      "iter=4 \t b=0.04904 \t W=0.26396 \t loss=1.5354491939869426\n",
      "iter=5 \t b=0.03017 \t W=0.28825 \t loss=1.5156918304474685\n",
      "iter=499996 \t b=-26.92465 \t W=9.06005 \t loss=0.0002400575751982959\n",
      "iter=499997 \t b=-26.92465 \t W=9.06006 \t loss=0.00024005709498780932\n",
      "iter=499998 \t b=-26.92466 \t W=9.06006 \t loss=0.00024005661477943257\n",
      "iter=499999 \t b=-26.92466 \t W=9.06006 \t loss=0.0002400561345726105\n",
      "iter=500000 \t b=-26.92467 \t W=9.06006 \t loss=0.00024005565436823126\n"
     ]
    }
   ],
   "source": [
    "# The small dataset\n",
    "data = np.array([[5,1],\n",
    "                 [4,1],\n",
    "                 [2,0]])\n",
    "col_names = ['num_bedrooms', 'is_easy_sell']\n",
    "print(pd.DataFrame(data, columns=col_names), \"\\n\")\n",
    "\n",
    "X = data[:, :-1] # all rows, all columns except the last column\n",
    "Y = data[:, -1]  # all rows, last column only\n",
    "\n",
    "#X, max_min_vals = max_min_norm(X) # normalize the input features\n",
    "X = X.T\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "# Initialize bias and weights\n",
    "initial_b = 0\n",
    "initial_W = 0\n",
    "#initial_W2 = 0\n",
    "\n",
    "# Set learing rate and iterations\n",
    "learning_rate = 0.1\n",
    "learning_iterations = 500000\n",
    "\n",
    "# Start the training of logistic regression model\n",
    "loss_history, b, W = train(X, Y, initial_b, np.array([initial_W]), learning_rate, learning_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f295ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
